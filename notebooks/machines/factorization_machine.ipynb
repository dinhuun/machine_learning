{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8ae6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import from_numpy\n",
    "\n",
    "from machine_learning.classes import OptimizerConfigs, TrainConfigs\n",
    "from machine_learning.machines.factorization_machine import ModelConfigs, FactorizationMachine, load, save, train\n",
    "from machine_learning.strings import sigmoid_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbca7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not a good dataset for factorization machine, as there is no interation between features\n",
    "n_samples = 10000\n",
    "n_features = 10\n",
    "centers = 2\n",
    "random_state = 0\n",
    "X, Y = make_blobs(n_samples, n_features, centers=centers, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "879267ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = X.astype(np.float32), Y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10e11a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, Y_temp, Y_test = train_test_split(X, Y, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b359540",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_temp, Y_temp, test_size=0.25, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12498401",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = from_numpy(X_train)\n",
    "Y_train_t = from_numpy(Y_train).view(-1, 1).float()\n",
    "X_val_t = from_numpy(X_val)\n",
    "Y_val_t = from_numpy(Y_val).view(-1, 1)\n",
    "X_test_t = from_numpy(X_test)\n",
    "Y_test_t = from_numpy(Y_test).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4072cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = 2\n",
    "activation_name = sigmoid_str\n",
    "\n",
    "model_configs = ModelConfigs(n_features=n_features, \n",
    "                             n_factors=n_factors, \n",
    "                             activation_name=activation_name,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad93ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FactorizationMachine(**model_configs.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831d2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "optimizer_configs = OptimizerConfigs(lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46548891",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "patience = 500\n",
    "\n",
    "train_configs = TrainConfigs(optimizer_configs=optimizer_configs, \n",
    "                             n_epochs=n_epochs,\n",
    "                             patience=patience,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b25a0380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss 60.66188430786133, val score 0.4775\n",
      "epoch 10, train loss 4.171582986600697e-06, val score 1.0\n",
      "epoch 20, train loss 0.25596821308135986, val score 0.994\n",
      "epoch 30, train loss 0.21405665576457977, val score 0.9975\n",
      "epoch 40, train loss 0.035855308175086975, val score 1.0\n",
      "epoch 50, train loss 0.017039833590388298, val score 1.0\n",
      "epoch 60, train loss 0.016666699200868607, val score 1.0\n",
      "epoch 70, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 80, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 90, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 100, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 110, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 120, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 130, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 140, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 150, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 160, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 170, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 180, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 190, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 200, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 210, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 220, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 230, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 240, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 250, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 260, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 270, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 280, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 290, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 300, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 310, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 320, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 330, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 340, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 350, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 360, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 370, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 380, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 390, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 400, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 410, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 420, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 430, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 440, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 450, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 460, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 470, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 480, train loss 0.01666666753590107, val score 1.0\n",
      "epoch 490, train loss 0.01666666753590107, val score 1.0\n"
     ]
    }
   ],
   "source": [
    "best_model, best_score, best_epoch = train(train_configs, model, X_train_t, Y_train_t, X_val_t, Y_val_t, verbose=True, verbose_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b480d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note val scores in the latter epochs are low and best val score happen way earlier. overfitting?\n",
    "best_score, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4feb3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"factorization_machine.pt\"\n",
    "save(best_model, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f08e686",
   "metadata": {},
   "source": [
    "### predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "593af011",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load(filepath)\n",
    "best_model.eval()\n",
    "Y_test_pred = best_model(X_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe64f9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score 100.00%\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(np.around(Y_test_pred.detach().numpy()), Y_test_t) * 100\n",
    "print(f\"accuracy score {score:0.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
